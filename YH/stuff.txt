I am putting my code here temporarily.

Interface.py isn't that important. It just provides a simple interface to do some manual configuration and stuff.
The main function still calls all the functions in core.py.

Core.py
This is where all the important stuff are.
So this is how I scrap the attachments and there are four base function that does that:
1. Initialise Browser (function)
It takes in user auth (email and password) and logins into outlook.
It uses selenium to work, which isn't an elegant solution but that's why we have zy.
It also transfer the session cookies to requests, which will be used on all other functions.

2. Obtain Courses (function)
It isn't very user friendly for users to find their course id by themselves. 
Hence, this function basically helps user to find their courses' ids
It returns something called a structure.

Structure (nested dictionary)
This object holds the structure of the attachments, so we can organise them into their appropriate folder.
It goes something like this.
{Course ID : { Name, Content Type, Content : { Content ID : { Name, Content Type, Content : {...} } } ] }, ...}

Sample:
###############################################################################################
{'_32389_1': {'content': {'_2050292_1': {'content': 'Dev: Uncatched Error - No '
                                                    'Result or Status '
                                                    'Returned.',
                                         'content type': 'folder',
                                         'name': 'Multiple Internships for '
                                                 'August 2018 Vacation'},
                          '_2050293_1': {'content': 'Dev: Uncatched Error - No '
                                                    'Result or Status '
                                                    'Returned.',
                                         'content type': 'folder',
                                         'name': 'Learning Materials'},
                          '_2319864_1': {'content': {...},
                                         'content type': 'folder',
                                         'name': 'Multiple Internships for '
                                                 'February 2019 Vacation'}},
              'content_type': 'course',
              'name': 'Multiple Internships Opportunities'},
 '_33367_1': {'content': {'_2069465_1': {'content': {...},
                                         'content type': 'folder',
                                         'name': 'Module Information'},
                          '_2069466_1': {'content': {...},
                                         'content type': 'folder',
                                         'name': 'Learning Materials'},
                          '_2154709_1': {'content': 'restricted',
                                         'content type': 'folder',
                                         'name': 'BCR Collaborate'},
                          '_2154710_1': {'content': 'restricted',
                                         'content type': 'folder',
                                         'name': 'Web Links'},
                          '_2154711_1': {'content': 'restricted',
                                         'content type': 'folder',
                                         'name': 'Assignments'}},
              'content_type': 'course',
              'name': 'ADVANCED STATISTICS(1211_AS_007901)'}}
###############################################################################################
The sample above shows the full structure, generated by the current function (2) and the next function (3).
The current function (that I am talking now) doesn't generate the whole structure, it generates the course part not the content part. 

3. Obtain Structure (function)
This functions takes in a partially generated structure, the one with only the course_id and gets the remaining structure (or content).
It uses requests and Blackboard API to obtain the meta data of the documents.
Meta data are the description of the content. Is it a folder or is it a document? 
This structure follows similar to how the Blackboard API returns the data. So do check out how the blackboard api works.
By this time, the selenium is not needed anymore.

4. Get Attachments (function)
From the generated structure, this function can now play it's part and recursively loop thru the structure (tree) and go to the corresponding website to scrap the corresponding content (which are the documents / attachments)

4a. Why not use BB API and download directly?
e.g. /learn/api/public/v1/courses/{courseId}/contents/{contentId}/attachments/{attachmentId}/download
Because for some reason, we don't have the permission to do so. IDK why. And that's why we need smart pp like u to fix it ;)
or can u? Maybe the sch block us for some reason? Whatever, we have to scrap the webpage and download from the given link.

4b. Patterns in the link generated?
I can't find one. 

4c. How does scrapping the attachment link works?
Go to MEL >> Click on a coure >> Click on another folder (learning materials) >> Inspect Element
The whole HTML Code is very densely nested. Upon further inspection, the mel webpage uses iframes for its interactive popups (also meaning, the url doesn't change as well). The thing is that the html content doesn't load up if you don't click on it. And once you click on another course, it closes the original html course content. The first approach would be to use selenium to click and load the neccessary element and then we scrap it individually. However, selenium is something we want to prevent using.
iframes seems like a thorn but then it tells us that we can actually go to the webpage the iframe is targetted from and scrap it from there! And it does works!
The url that iframes is pointed to has a pattern as well!
In python, the string format would be like this: 
'https://mel.np.edu.sg/webapps/blackboard/content/listContent.jsp?course_id={}&content_id={}', where we need to insert the brace ('{}')
with it's correspounding ids. The ids will be given by the structure object as mentioned eariler.
Hence, requests goes to the required webpage (which is static) and scrap the whole HTML content.
It passes this data to Beautiful Soup.
Given the IDs of the attachments, Beautiful Soup will get_element_by_id, and find the link associated with the attachment.

And TADA.
It creates the neccessary folder as it goes on, and downloads the required attachment.

Notes:
It might seem like a bad thing that we couldn't get the api to download the attachments, but it may not be a bad thing.
You know as you go down the structure tree, we have more and more folders and attachments to scrap at each level. I run a test to check how many api requests and downloads are made:
api_count = 98
download_count = 205
If we were to ask bb api to download the attachment we would be making another 205 api calls!
I am not sure, but there must be an BB API must have a cap to how many times we can call the api, right?
A 100K / 24 hrs?
https://community.blackboard.com/docs/DOC-4258-developer-groups-site-quotas-and-rate-limits
100k / 100 = 1000 pp
100k / 300 = 333 pp
